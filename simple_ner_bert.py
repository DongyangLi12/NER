# -*- coding: utf-8 -*-
"""Simple_NER_BERT

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1byOKxXJPDeoDzKblV4kw_RHsISRxE8CE
"""

pip install scikit-learn

from collections import defaultdict
from urllib import request
import json
import pandas as pd
from sklearn.metrics import f1_score
from math import ceil
from transformers import AutoModel, AutoTokenizer
from tqdm.auto import tqdm

import random
from random import shuffle
import numpy as np
import torch
import torch.nn as nn

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

def parse_conllu_using_pandas(block):
    records = []
    for line in block.splitlines():
        if not line.startswith('#'):
            records.append(line.strip().split('\t'))
    return pd.DataFrame.from_records(
        records,
        columns=['ID', 'FORM', 'TAG', 'Misc1', 'Misc2'])

def tokens_to_labels(df):
    return (
        df.FORM.tolist(),
        df.TAG.tolist()
    )

PREFIX = "https://raw.githubusercontent.com/UniversalNER/"
DATA_URLS = {
    "en_ewt": {
        "train": "UNER_English-EWT/master/en_ewt-ud-train.iob2",
        "dev": "UNER_English-EWT/master/en_ewt-ud-dev.iob2",
        "test": "UNER_English-EWT/master/en_ewt-ud-test.iob2"
    },
    "en_pud": {
        "test": "UNER_English-PUD/master/en_pud-ud-test.iob2"
    }
}

# en_ewt is the main train-dev-test split
# en_pud is the OOD test set
data_dict = defaultdict(dict)
for corpus, split_dict in DATA_URLS.items():
    for split, url_suffix in split_dict.items():
        url = PREFIX + url_suffix
        with request.urlopen(url) as response:
            txt = response.read().decode('utf-8')
            data_frames = map(parse_conllu_using_pandas,
                              txt.strip().split('\n\n'))
            token_label_alignments = list(map(tokens_to_labels,
                                              data_frames))
            data_dict[corpus][split] = token_label_alignments

# Saving the data so that you don't have to redownload it each time.
with open('ner_data_dict.json', 'w', encoding='utf-8') as out:
    json.dump(data_dict, out, indent=2, ensure_ascii=False)

with open('ner_data_dict.json', 'r', encoding='utf-8') as f:
    data_dict = json.load(f)

"""Organize the input as list of 'word-tag' pairs"""

train_set = [[[word, label] for word, label in zip(words, labels)] for words, labels in data_dict['en_ewt']['train']]
dev_set = [[[word, label] for word, label in zip(words, labels)] for words, labels in data_dict['en_ewt']['dev']]
test_set = [[[word, label] for word, label in zip(words, labels)] for words, labels in data_dict['en_ewt']['test']]
test_set_ood =  [[[word, label] for word, label in zip(words, labels)] for words, labels in data_dict['en_pud']['test']]

labels = set()
for ex in train_set:

    labels.update([el[1] for el in ex])
n_classes = len(labels)
sorted(labels)

label_to_i = {
    label: i
    for i, label in enumerate(sorted(labels))
}
i_to_label = {
    i: label
    for label, i in label_to_i.items()
}

label_to_i

label_map = {
    'B-LOC': 'B',
    'I-LOC': 'I',
    'B-PER': 'B',
    'I-PER': 'I',
    'B-ORG': 'B',
    'I-ORG': 'I',
    'O': 'O'
}

train_set_simple = []
dev_set_simple = []
test_set_simple = []
test_set_ood_simple = []

for sentence in train_set:
    simplified_sentence_train = [[word, label_map[label]] for word, label in sentence]
    train_set_simple.append(simplified_sentence_train)
for sentence in dev_set:
    simplified_sentence_dev = [[word, label_map[label]] for word, label in sentence]
    dev_set_simple.append(simplified_sentence_dev)
for sentence in test_set:
    simplified_sentence_test = [[word, label_map[label]] for word, label in sentence]
    test_set_simple.append(simplified_sentence_test)
for sentence in test_set_ood:
    simplified_sentence_test_ood = [[word, label_map[label]] for word, label in sentence]
    test_set_ood_simple.append(simplified_sentence_test_ood)

train_set_simple[0], train_set[0]

model_tag = 'google-bert/bert-base-uncased'

tokeniser = AutoTokenizer.from_pretrained(model_tag)

labels_simple = set()
for ex in train_set_simple:

    labels_simple.update([el[1] for el in ex])
n_classes_simple = len(labels_simple)
sorted(labels_simple)


label_to_i_simple = {
    label: i
    for i, label in enumerate(sorted(labels_simple))
}
i_to_label_simple = {
    i: label
    for label, i in label_to_i_simple.items()
}

label_to_i_simple

class ClassificationHeadSimple(nn.Module):
    def __init__(self, model_dim=768, n_classes_simple=3, dropout_prob=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout_prob)
        self.linear  = nn.Linear(model_dim, n_classes_simple)

    def forward(self, x):
        x = self.dropout(x)
        return self.linear(x)

def process_sentence_batch_simple(batch_inputs, label_to_i_simple,
                           tokeniser, encoder, clf_head,
                           encoder_device, clf_head_device):

    # process all sentences in the batch
    gold_labels = []
    words_list = []
    all_first_subword_embeddings = []


    for sentence in batch_inputs:
        sentence_labels = [label_to_i_simple[label] for _, label in sentence]
        gold_labels.append(sentence_labels)

        words = [word for word, _ in sentence]
        words_list.append(words)


    tokenization = tokeniser(
        words_list,
        is_split_into_words=True,
        padding='longest',
        truncation=False,
        return_tensors='pt'
    )


    inputs = {k: v.to(encoder_device) for k, v in tokenization.items()}

    # delete [CLS] and [SEP]
    outputs = encoder(**inputs).last_hidden_state[:, 1:-1, :]

    # extract the first subword of each word
    for i in range(len(batch_inputs)):
        word_ids = tokenization.word_ids(batch_index=i)
        seen = set()
        first_sub_embs = []
        for j, wid in enumerate(word_ids[1:-1]):
            if wid is None or wid in seen:
                continue
            first_sub_embs.append(outputs[i, j])
            seen.add(wid)

        sent_emb = torch.stack(first_sub_embs)
        all_first_subword_embeddings.append(sent_emb)


    gold_label_tensors = [
        torch.tensor(lbls, dtype=torch.long).to(clf_head_device)
        for lbls in gold_labels
    ]

    # feed to the classfication head sentence by sentence
    logits_list = []
    for sent_emb in all_first_subword_embeddings:
        # sent_emb: (num_words, hidden_size)
        logits = clf_head(sent_emb.to(clf_head_device))
        # logits: (num_words, num_classes)
        logits_list.append(logits)

    return logits_list, gold_label_tensors

def train_epoch_simple(train_data, label_to_i_simple, batch_size, tokeniser, encoder, clf_head,
                encoder_device, clf_head_device, loss_fn, optimiser):
    encoder.train()
    n_steps = ceil(len(train_data) / batch_size)
    if len(train_data) - len(train_data) // batch_size * batch_size:
        n_steps -= 1
    epoch_losses = torch.empty(n_steps)
    for step_n in tqdm(range(n_steps), leave=False, desc='Train'):
       lo = step_n * batch_size
       hi = lo + batch_size
       batch_texts = train_data[lo:hi]

       logits_list, gold_label_tensors = process_sentence_batch_simple(batch_texts, label_to_i_simple,
                           tokeniser, encoder, clf_head,
                           encoder_device, clf_head_device)

       # concatenate all the logits
       batch_logits = torch.cat(logits_list, dim=0)
       # concatenate all the labels
       batch_labels = torch.cat(gold_label_tensors, dim=0)

       loss = loss_fn(batch_logits, batch_labels)
       loss.backward()
       optimiser.step()
       optimiser.zero_grad()
       epoch_losses[step_n] = loss.item()
    return epoch_losses.mean().item()

def get_spans_simple(label_ids, i_to_label_simple):
    labeled = []
    unlabeled = []
    start_lab = None
    start_unlab = None

    for i, lid in enumerate(label_ids):
        tag = i_to_label_simple[lid]
        # labeled span:
        if tag == 'B':
            if start_lab is not None:
                labeled.append((start_lab, i-1))
            start_lab = i
        elif tag == 'I':
            if start_lab is None:
                pass
        else:
            if start_lab is not None:
                labeled.append((start_lab, i-1))
                start_lab = None
        # unlabeled span:
        if tag != 'O':
            if start_unlab is None:
                start_unlab = i
        else:
            if start_unlab is not None:
                unlabeled.append((start_unlab, i-1))
                start_unlab = None

    if start_lab is not None:
        labeled.append((start_lab, len(label_ids)-1))
    if start_unlab is not None:
        unlabeled.append((start_unlab, len(label_ids)-1))

    return labeled, unlabeled

def validate_epoch_simple(dev_inputs, label_to_i_simple, tokeniser,
                          encoder, clf_head,
                          encoder_device, clf_head_device,
                          batch_size, i_to_label_simple):

    encoder.eval()

    all_true_labels = []
    all_pred_labels = []

    total_true_lab_spans = 0
    total_true_unlab_spans = 0
    total_pred_lab_spans = 0
    total_pred_unlab_spans = 0

    correct_unlab    = 0
    correct_lab      = 0

    n_steps = ceil(len(dev_inputs) / batch_size)
    for step in tqdm(range(n_steps), desc="Eval", leave=False):
        lo, hi = step * batch_size, (step + 1) * batch_size
        batch = dev_inputs[lo:hi]
        with torch.no_grad():
            logits_list, gold_list = process_sentence_batch_simple(
                batch, label_to_i_simple,
                tokeniser, encoder, clf_head,
                encoder_device, clf_head_device
            )
        for logits, gold in zip(logits_list, gold_list):
            pred_ids = torch.argmax(logits, dim=-1).cpu().numpy().tolist()
            true_ids = gold.cpu().numpy().tolist()
            # collect token-level tags
            all_pred_labels.extend(pred_ids)
            all_true_labels.extend(true_ids)
            # extract labeled and unlabeled spans
            true_labeled, true_unlabeled = get_spans_simple(true_ids, i_to_label_simple)
            pred_labeled, pred_unlabeled = get_spans_simple(pred_ids, i_to_label_simple)

            total_true_lab_spans += len(true_labeled)
            total_true_unlab_spans += len(true_unlabeled)
            total_pred_lab_spans += len(pred_labeled)
            total_pred_unlab_spans += len(pred_unlabeled)

            correct_lab   += len(set(true_labeled) & set(pred_labeled))
            correct_unlab += len(set(true_unlabeled) & set(pred_unlabeled))


    # labeled and unlabeled span scores
    unlabeled_span_score = correct_unlab / total_true_unlab_spans if total_true_unlab_spans else 0.0
    labeled_span_score   = correct_lab   / total_true_lab_spans if total_true_lab_spans else 0.0

    # labeled span F1
    precision_span = correct_lab / total_pred_lab_spans if total_pred_lab_spans else 0.0
    recall_span    = correct_lab / total_true_lab_spans if total_true_lab_spans else 0.0
    f1_span        = 2 * precision_span * recall_span / (precision_span + recall_span) if (precision_span + recall_span) else 0.0

    # token-level F1 per tag and macro
    labels = [label_to_i_simple['B'], label_to_i_simple['I'], label_to_i_simple['O']]
    f1_per_tag_vals = f1_score(all_true_labels, all_pred_labels,
                                labels=labels, average=None)
    f1_per_tag = {i_to_label_simple[l]: f for l, f in zip(labels, f1_per_tag_vals)}
    macro_f1 = f1_score(all_true_labels, all_pred_labels,
                               labels=labels, average='macro')


    return (unlabeled_span_score,
            labeled_span_score,
            f1_span,
            f1_per_tag,
            macro_f1)

encoder_device = 0
encoder = AutoModel.from_pretrained(
    model_tag).to(encoder_device)
# NB: pass the number of different POS tags
clf_head = ClassificationHeadSimple(n_classes_simple=n_classes_simple)
clf_head_device = 0
clf_head.to(clf_head_device);

batch_size = 16
n_epochs   = 8
patience   = 2
best_f1    = 0.0
wait       = 0

loss_fn = nn.CrossEntropyLoss()
optimiser = torch.optim.AdamW(
    list(encoder.parameters()) + list(clf_head.parameters()),
    lr=1e-5
)

for epoch in range(1, n_epochs+1):

    shuffle(train_set_simple)
    train_loss = train_epoch_simple(
        train_set_simple, label_to_i_simple, batch_size,
        tokeniser, encoder, clf_head,
        encoder_device, clf_head_device,
        loss_fn, optimiser
    )
    print(f"Epoch {epoch} train loss: {train_loss:.4f}")

    (unlabeled_span_score,
     labeled_span_score,
     f1_span,
     f1_per_tag,
     macro_f1) = validate_epoch_simple(dev_set_simple, label_to_i_simple,
                                           tokeniser, encoder, clf_head,
                                           encoder_device, clf_head_device,
                                           batch_size, i_to_label_simple)


    print(f"Unlabeled span score: {unlabeled_span_score:.4f}")
    print(f"Labeled   span score: {labeled_span_score:.4f}")
    print(f"Labeled   span F1:    {f1_span:.4f}")
    for tag, f1v in f1_per_tag.items():
        print(f"Token-level  {tag}: F1 = {f1v:.4f}")
    print(f"Token-level Macro F1: {macro_f1:.4f}")

    # Early Stopping
    if f1_span > best_f1:
        best_f1 = f1_span
        wait = 0
        torch.save({
            'encoder': encoder.state_dict(),
            'clf_head': clf_head.state_dict(),
            'optim': optimiser.state_dict(),
        }, 'best_model.pt')
        print(f"→ New best span F1: {best_f1:.4f}, model saved.")
    else:
        wait += 1
        print(f"→ No improvement for {wait} epoch(s).")
        if wait >= patience:
            print(f"→ Early stopping at epoch {epoch}.")
            break

checkpoint = torch.load('best_model.pt',
                        map_location=lambda storage, loc: storage)
encoder.load_state_dict( checkpoint['encoder'] )
clf_head.load_state_dict( checkpoint['clf_head'] )
optimiser.load_state_dict( checkpoint['optim'] )


encoder.to(encoder_device).eval()
clf_head.to(clf_head_device).eval()


def test_on_split(name, dataset):
    unlab, lab, f1_span, token_f1_per_tag, macro_token_f1 = validate_epoch_simple(
        dev_inputs      = dataset,
        label_to_i_simple = label_to_i_simple,
        tokeniser       = tokeniser,
        encoder         = encoder,
        clf_head        = clf_head,
        encoder_device  = encoder_device,
        clf_head_device = clf_head_device,
        batch_size      = batch_size,
        i_to_label_simple = i_to_label_simple
    )
    print(f"\n=== {name} ===")
    print(f"Unlabeled span match:       {unlab:.4f}")
    print(f"Labeled   span match:       {lab:.4f}")
    print(f"Span‐level labeled F1:      {f1_span:.4f}")
    print("Token‐level F1 per tag:")
    for tag, v in token_f1_per_tag.items():
        print(f"  {tag}: {v:.4f}")
    print(f"Token‐level Macro F1 (7):   {macro_token_f1:.4f}")

test_on_split("In-Domain Dev Set",       test_set_simple)
test_on_split("Out-of-Domain Dev Set",   test_set_ood_simple)